services:
  frontend:
    build:
      context: ./frontend
      dockerfile: ../Dockerfile.js
    ports:
      - "59001:59001"
    volumes:
      - ./frontend:/app
  backend: &backend-base
    build:
      context: ./backend
      dockerfile: ../Dockerfile.python
    ports:
      - "59002:59002"
    environment:
      AF_DB_PATH: /app/save.db
      AF_DB_KEY: ${AF_DB_KEY}
    volumes:
      - ./backend:/app
      - ${HOME}/.cache:/.cache
  backend-llm-cuda:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    <<: *backend-base
    build:
      context: ./backend
      dockerfile: ../Dockerfile.python
      args:
        UV_EXTRA: llm-cuda
    ports:
      - "59002:59002" # conflicts with default backend
    environment:
      - UV_EXTRA=llm-cuda
    profiles:
      - llm-cuda
  backend-llm-amd:
    <<: *backend-base
    build:
      context: ./backend
      dockerfile: ../Dockerfile.python
      args:
        UV_EXTRA: llm-amd
    ports:
      - "59002:59002" # conflicts with default backend
    environment:
      - UV_EXTRA=llm-amd
    profiles:
      - llm-amd
  backend-llm-cpu:
    <<: *backend-base
    build:
      context: ./backend
      dockerfile: ../Dockerfile.python
      args:
        UV_EXTRA: llm-cpu
    ports:
      - "59002:59002" # conflicts with default backend
    environment:
      - UV_EXTRA=llm-cpu
    profiles:
      - llm-cpu
